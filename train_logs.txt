raw: 与searchr1源码超参一致
    现象记录：
    actor：
        actor的entropy_loss 稳定下降
        gradnorm在后期出现了梯度爆炸
        pg_clipfrac：逐步上升
        pg_loss在后期出现非常大的值
        ppo_kl开始很小，后期逐步不稳定——新分布背离旧分布的程度，更新幅度
        后期训练崩溃
    critic：
        不存在梯度爆炸现象
        advantages——前期优势相对稳定，后期优势变化幅度变大
        kl——critic模型和参考模型的分歧程度，后期逐步变大
        returns——蒙特卡洛式的 Q 值估计 变大， 一定程度说明”奖励崩塌“
        rewards——后期逐步崩塌
        score——后期奖励向0进发，奖励逐步崩塌
        values——动作价值函数，相对正常，后期可能增大（由超参数而定）
        vf-clip频率逐步降低
        loss相对稳定
        vf_explained_var——后期逐步接近于1，价值函数越来越能解释rewards的方差
    envs：
        finish_ratio：有稳定完成到不稳定
        number_of_actions：轮数有变少趋势
        number_of_valid_action：每条样本的平均有效action数目变少
        number_of_valid_search：每条样本有效有效search的数目变少
        ratio_of_valid_action：逐步变少

0616: 相对于raw，修改了gae的超参，调整了
    algorithm.gamma=0.99 
    algorithm.lam=0.95 
    前期训练比raw要更好，但是策略崩塌到来的更早
0618: 
    1、给critic和actor添加超参数 lr_warmup_direction
        actor学习率逐步衰减，而不是warmup
        critic保持不变
        train_ppo_0618_1.sh 
    2、添加searchllm相关参数
        使用api llm作为搜索引擎 使用zhipu ai的glm-4-air-250414
        因为api llm只能单batch索引，为了训练达到一定的trade-off，减小了一些batch
        对超参数整体进行了一定修改：最大步数设置为了300步，以及一些其他的超参数
        0618_2和0618_3的区别：一个do_search，一个没有do_search
0621-0622:
    1、想要复现出来0618的结果 train_ppo_0621.sh 区别是step步数变小，critic的warmup步数也同步变化
    2、train_ppo_0622.sh——大杂烩，融入所有自己认为合理的操作
        1）batch size变小,step变少
        2）学习率模式改为warmup_decay
        3）critic启动warmup，前几个回合不更新actor,critic_warmup=10
        4）lr_warmup_steps_ratio 改为actor/critic 0.25/0.15
        5）algorithm.lam=0.999 降低一些方差

    